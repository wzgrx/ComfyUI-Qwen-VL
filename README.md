# ComfyUI-Qwen-VL

[English](README.md#english-version) | [ä¸­æ–‡](README.md#chinese-version)

## Chinese Version

### ComfyUI-Qwen-VL

Qwen-VLç³»åˆ—å¤§è¯­è¨€æ¨¡å‹çš„ComfyUIæ‰©å±•ï¼Œæ”¯æŒæ–‡æœ¬ç”Ÿæˆã€å›¾åƒç†è§£ã€è§†é¢‘åˆ†æç­‰å¤šæ¨¡æ€åŠŸèƒ½ã€‚

### ç‰¹ç‚¹

- æ”¯æŒQwen2-VLã€Qwen2.5-VLç­‰ç³»åˆ—æ¨¡å‹
- æä¾›æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç†è§£ã€è§†é¢‘åˆ†æç­‰å¤šç§åŠŸèƒ½èŠ‚ç‚¹
- æ”¯æŒæ¨¡å‹é‡åŒ–é…ç½®ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
- æä¾›ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ï¼Œä¾¿äºå‚æ•°è°ƒæ•´

### å®‰è£…

1. æ‰“å¼€ComfyUIçš„custom_nodesç›®å½•
2. å…‹éš†æ­¤ä»“åº“ï¼š
   ```bash
   git clone https://github.com/SXQBW/ComfyUI-Qwen-VL.git
   ```
3. å®‰è£…ä¾èµ–ï¼š
   ```bash
   cd ComfyUI-Qwen-VL
   pip install -r requirements.txt
   ```
4. é‡å¯ComfyUI

### ä½¿ç”¨æ–¹æ³•

1. åœ¨ComfyUIç•Œé¢ä¸­ï¼Œæ‰¾åˆ°Qwen-VLç›¸å…³èŠ‚ç‚¹
2. é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹å’Œé‡åŒ–æ–¹å¼
3. é…ç½®ç”Ÿæˆå‚æ•°ï¼Œå¦‚æ¸©åº¦ã€æœ€å¤§ä»¤ç‰Œæ•°ç­‰
4. è¿æ¥è¾“å…¥ï¼ˆæ–‡æœ¬ã€å›¾åƒæˆ–è§†é¢‘ï¼‰å’Œè¾“å‡ºèŠ‚ç‚¹
5. è¿è¡Œå·¥ä½œæµ

### æ¨¡å‹æ”¯æŒ

ç›®å‰æ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š

- Qwen/Qwen2.5-VL-3B-Instruct
- Qwen/Qwen2.5-VL-3B-Instruct-AWQ
- Qwen/Qwen2.5-VL-7B-Instruct
- Qwen/Qwen2.5-VL-7B-Instruct-AWQ
- Qwen/Qwen2.5-VL-32B-Instruct
- Qwen/Qwen2.5-VL-32B-Instruct-AWQ
- Qwen/Qwen2.5-VL-72B-Instruct
- Qwen/Qwen2.5-VL-72B-Instruct-AWQ
- Qwen/Qwen2-VL-2B
- Qwen/Qwen2-VL-2B-Instruct
- Qwen/Qwen2-VL-7B-Instruct
- Qwen/Qwen2-VL-72B-Instruct
- Qwen/Qwen2-VL-2B-Instruct-AWQ
- Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4
- Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8
- Qwen/Qwen2-VL-7B-Instruct-AWQ
- Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4
- Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8
- Qwen/Qwen2-VL-72B-Instruct-AWQ
- Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4
- Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8
- huihui-ai/Qwen2.5-VL-7B-Instruct-abliterated

### é‡åŒ–é€‰é¡¹

- ğŸš« None (Original Precision): ä½¿ç”¨åŸå§‹ç²¾åº¦
- ğŸ‘ 4-bit (VRAM-friendly): ä½¿ç”¨4ä½é‡åŒ–ï¼ŒèŠ‚çœæ˜¾å­˜
- âš–ï¸ 8-bit (Balanced Precision): ä½¿ç”¨8ä½é‡åŒ–ï¼Œå¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½

### ç¤ºä¾‹å·¥ä½œæµ

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å›¾åƒç†è§£å·¥ä½œæµç¤ºä¾‹ï¼š

![alt text](pic/screenshot-20250506-055913.png)

![alt text](pic/screenshot-20250506-065011.png)
### å¸¸è§é—®é¢˜

#### æ¨¡å‹åŠ è½½é—®é¢˜

å¦‚æœé‡åˆ°æ¨¡å‹åŠ è½½é”™è¯¯ï¼Œè¯·ç¡®ä¿ï¼š

1. æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®
2. æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼ˆè¯·æ ¹æ®æ˜¾å­˜çš„å¤§å°é€‰æ‹©åˆé€‚æ¨¡å‹ï¼Œåˆ«ä¸Šæ¥å°±ç›´å¥”72Bæ¨¡å‹ï¼Œå¤§åŠ›å‡ºä¸äº†å¥‡è¿¹ï¼Œåªä¼šçˆ†æ˜¾å­˜ï¼‰
3. å·²å®‰è£…æ‰€æœ‰å¿…è¦ä¾èµ–

#### å…³äºé‡åŒ–

å½“ä½¿ç”¨é¢„é‡åŒ–æ¨¡å‹ï¼ˆå¦‚AWQç‰ˆæœ¬ï¼‰æ—¶ï¼Œå¯èƒ½ä¼šçœ‹åˆ°ä»¥ä¸‹è­¦å‘Šï¼šæ¨¡å‹ Qwen2.5-VL-3B-Instruct-AWQ å·²ç»æ˜¯é‡åŒ–æ¨¡å‹ï¼Œå°†å¿½ç•¥ç”¨æˆ·çš„é‡åŒ–è®¾ç½®è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œæ’ä»¶ä¼šè‡ªåŠ¨ä½¿ç”¨æ¨¡å‹çš„é¢„é‡åŒ–ç‰ˆæœ¬ã€‚

### è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºå»ºè®®ï¼



### è‡´è°¢

æ„Ÿè°¢Qwenå›¢é˜Ÿå¼€å‘çš„å¼ºå¤§æ¨¡å‹ï¼Œä»¥åŠComfyUIç¤¾åŒºçš„æ”¯æŒï¼


## English Version

### ComfyUI-Qwen-VL

A ComfyUI extension for Qwen-VL series large language models, supporting multi-modal functions such as text generation, image understanding, and video analysis.

### Features

- Support for Qwen2-VL, Qwen2.5-VL and other series models
- Provides various functional nodes for text generation, image understanding, video analysis, etc.
- Supports model quantization configuration to optimize memory usage
- Intuitive user interface for easy parameter adjustment

### Installation

1. Open the custom_nodes directory of ComfyUI
2. Clone this repository:
   ```bash
   git clone https://github.com/SXQBW/ComfyUI-Qwen-VL.git
   ```
3. Install dependencies:
   ```bash
   cd ComfyUI-Qwen-VL
   pip install -r requirements.txt
   ```
4. Restart ComfyUI

### Usage

1. In the ComfyUI interface, find the Qwen-VL related nodes
2. Select the model and quantization method to use
3. Configure generation parameters such as temperature, maximum tokens, etc.
4. Connect inputs (text, image or video) and output nodes
5. Run the workflow

### Model Support

Currently supports the following models:

- Qwen/Qwen2.5-VL-3B-Instruct
- Qwen/Qwen2.5-VL-3B-Instruct-AWQ
- Qwen/Qwen2.5-VL-7B-Instruct
- Qwen/Qwen2.5-VL-7B-Instruct-AWQ
- Qwen/Qwen2.5-VL-32B-Instruct
- Qwen/Qwen2.5-VL-32B-Instruct-AWQ
- Qwen/Qwen2.5-VL-72B-Instruct
- Qwen/Qwen2.5-VL-72B-Instruct-AWQ
- Qwen/Qwen2-VL-2B
- Qwen/Qwen2-VL-2B-Instruct
- Qwen/Qwen2-VL-7B-Instruct
- Qwen/Qwen2-VL-72B-Instruct
- Qwen/Qwen2-VL-2B-Instruct-AWQ
- Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4
- Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8
- Qwen/Qwen2-VL-7B-Instruct-AWQ
- Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4
- Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8
- Qwen/Qwen2-VL-72B-Instruct-AWQ
- Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4
- Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8
- huihui-ai/Qwen2.5-VL-7B-Instruct-abliterated

### Quantization Options

- ğŸš« None (Original Precision): Use original precision
- ğŸ‘ 4-bit (VRAM-friendly): Use 4-bit quantization to save VRAM
- âš–ï¸ 8-bit (Balanced Precision): Use 8-bit quantization for balanced precision and performance

### Example Workflow

Here's a simple example workflow for image understanding:

![alt text](pic/screenshot-20250506-055913.png)

![alt text](pic/screenshot-20250506-065011.png)

### FAQ

#### Model Loading Issues

If you encounter model loading errors, please ensure:

1. The model file path is correct
2. Ensure adequate GPU memory (select a model matching your VRAM capacity. Avoid using the 72B model directly unless confirmed - insufficient VRAM may cause crashes)
3. All necessary dependencies are installed

#### Quantization Warnings

When using a pre-quantized model (such as AWQ versions), you may see the following warning:The model Qwen2.5-VL-3B-Instruct-AWQ is already quantized. User quantization settings will be ignored.This is normal. The plugin will automatically use the pre-quantized version of the model.

### Contribution

Contributions, issues and feature requests are welcome!


### Acknowledgments

Thanks to the Qwen team for developing such powerful models, and to the ComfyUI community for their support!
    